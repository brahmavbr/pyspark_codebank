"""
Below snippet will help to fill forward value where value is either Null or 0
"""

from pyspark import SparkContext
from pyspark.sql import SQLContext
from pyspark.sql import functions as F
from pyspark.sql.window import Window

d = [{'key': 1, 'ts': 1,'id':500}, 
     {'key': 1, 'ts': 2,'id':0},
     {'key': 1, 'ts': 4, 'id': 6}, 
     {'key': 1, 'ts': 5},
     {'key': 1, 'ts': 6},
     {'key': 1, 'ts': 7, 'id': 20},
     {'key': 1, 'ts': 8},
     {'key': 1, 'ts': 9},
     {'key': 1, 'ts': 10},
     {'key': 1, 'ts': 11},
     {'key': 1, 'ts': 12, 'id': 5},
     {'key': 1, 'ts': 13}]
     
df = spark.createDataFrame(d)

df.show()
#will dispaly output as below"
"""
+----+---+---+
|  id|key| ts|
+----+---+---+
| 500|  1|  1|
|   0|  1|  2|
|   6|  1|  4|
|null|  1|  5|
|null|  1|  6|
|  20|  1|  7|
|null|  1|  8|
|null|  1|  9|
|null|  1| 10|
|null|  1| 11|
|   5|  1| 12|
|null|  1| 13|
+----+---+---+

"""

df.withColumn("id1", func.last(func.when(df.id==0,None).otherwise(df.id), True)
                      .over(Window.partitionBy('key').orderBy('ts').rowsBetween(-sys.maxsize, 0)))

#Output for above step
"""
+----+---+---+---+                                                              
|  id|key| ts|id1|
+----+---+---+---+
| 500|  1|  1|500|
|   0|  1|  2|500|
|   6|  1|  4|  6|
|null|  1|  5|  6|
|null|  1|  6|  6|
|  20|  1|  7| 20|
|null|  1|  8| 20|
|null|  1|  9| 20|
|null|  1| 10| 20|
|null|  1| 11| 20|
|   5|  1| 12|  5|
|null|  1| 13|  5|
+----+---+---+---+

"""
